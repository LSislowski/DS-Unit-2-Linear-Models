{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_214_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ZiquQUY6Rp"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dxN_Oq6Y6Rq"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DmgddtWY6Rr"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCSzV7qIY6Rr"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6xNISPebvPg",
        "outputId": "c049eeeb-db9e-4235-d4b7-b5d55495db1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Most IMPORTantly\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
        "from sklearn.pipeline import make_pipeline\n",
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1XxWq_4Y6Rr"
      },
      "source": [
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood'])\n",
        "    \n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['Rec', 'overall'])\n",
        "\n",
        "    # Drop columns related to social media \n",
        "    # Since we are looking for Great Burritos only\n",
        "    df = df.drop(columns=['Yelp', 'Google'])\n",
        "    \n",
        "    # Drop Columns that are not included in less than ~2% of burritos\n",
        "    cat_cutoff = 415\n",
        "    cols_to_drop = [col for col in df.select_dtypes('object').columns if df[col].isnull().sum() > cat_cutoff]\n",
        "    df.drop(columns= cols_to_drop, inplace=True)\n",
        "\n",
        "    # Remove other redundant or unecessary categories\n",
        "    # We are dropping length and circumference since they are used to calculate Volume\n",
        "    df = df.drop(columns=['Chips', 'Mass (g)', 'Density (g/mL)', 'Unreliable', 'Fries', 'Queso', 'Length', 'Circum', 'NonSD', 'Reviewer'] )\n",
        "    \n",
        "    # Write a function to turn NaN values into zeros and x/X values into ones\n",
        "    df = df.fillna(0)\n",
        "    df = df.replace(['X', 'x'], 1)\n",
        "    #df['Volume'] = df['Volume'].replace(0, np.NaN)\n",
        "\n",
        "    df = df.drop(columns= ['Burrito'])\n",
        "\n",
        "    # ----------STRETCH GOAL------------- \n",
        "    # ohe = OneHotEncoder(use_cat_names=True)\n",
        "    # df = ohe.fit_transform(df)\n",
        "    \n",
        "    # SimpleImputer().fit(df)\n",
        "    \n",
        "    \n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt2Wy0sbY6Rs"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ovTBvmY6Rs"
      },
      "source": [
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "df = wrangle(filepath)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgdj0up9Y6Rs"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi2SqImOY6Rs",
        "outputId": "f6e30e84-9be4-448e-b4b1-047fc4412b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cost</th>\n",
              "      <th>Hunger</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Tortilla</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Meat</th>\n",
              "      <th>Fillings</th>\n",
              "      <th>Meat:filling</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Salsa</th>\n",
              "      <th>Synergy</th>\n",
              "      <th>Wrap</th>\n",
              "      <th>Beef</th>\n",
              "      <th>Pico</th>\n",
              "      <th>Guac</th>\n",
              "      <th>Cheese</th>\n",
              "      <th>Sour cream</th>\n",
              "      <th>Pork</th>\n",
              "      <th>Chicken</th>\n",
              "      <th>Shrimp</th>\n",
              "      <th>Fish</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Beans</th>\n",
              "      <th>Lettuce</th>\n",
              "      <th>Tomato</th>\n",
              "      <th>Bell peper</th>\n",
              "      <th>Cabbage</th>\n",
              "      <th>Sauce</th>\n",
              "      <th>Salsa.1</th>\n",
              "      <th>Cilantro</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Pineapple</th>\n",
              "      <th>Avocado</th>\n",
              "      <th>Great</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-18</th>\n",
              "      <td>6.49</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>5.45</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>4.85</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>5.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-27</th>\n",
              "      <td>6.59</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cost  Hunger  Volume  Tortilla  ...  Onion  Pineapple  Avocado  Great\n",
              "Date                                        ...                                  \n",
              "2016-01-18  6.49     3.0     0.0       3.0  ...      0          0        0      0\n",
              "2016-01-24  5.45     3.5     0.0       2.0  ...      0          0        0      0\n",
              "2016-01-24  4.85     1.5     0.0       3.0  ...      0          0        0      0\n",
              "2016-01-24  5.25     2.0     0.0       3.0  ...      0          0        0      0\n",
              "2016-01-27  6.59     4.0     0.0       4.0  ...      0          0        0      1\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-m-S9WY6Rt"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XBBmJIpY6Rt",
        "outputId": "246368bb-56c5-4e24-bdd3-8e52ec34f0b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\n",
        "df.head()\n",
        "\n",
        "# Look for Null values \n",
        "df.info()\n",
        "df.isna().sum().sort_values(ascending=False)\n",
        "\n",
        "df['Volume'].value_counts().sort_values()\n",
        "df['Volume'].isna().sum()\n",
        "df['Great'].value_counts(normalize=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 421 entries, 2016-01-18 to 2019-08-27\n",
            "Data columns (total 34 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Cost          421 non-null    float64\n",
            " 1   Hunger        421 non-null    float64\n",
            " 2   Volume        421 non-null    float64\n",
            " 3   Tortilla      421 non-null    float64\n",
            " 4   Temp          421 non-null    float64\n",
            " 5   Meat          421 non-null    float64\n",
            " 6   Fillings      421 non-null    float64\n",
            " 7   Meat:filling  421 non-null    float64\n",
            " 8   Uniformity    421 non-null    float64\n",
            " 9   Salsa         421 non-null    float64\n",
            " 10  Synergy       421 non-null    float64\n",
            " 11  Wrap          421 non-null    float64\n",
            " 12  Beef          421 non-null    int64  \n",
            " 13  Pico          421 non-null    int64  \n",
            " 14  Guac          421 non-null    int64  \n",
            " 15  Cheese        421 non-null    int64  \n",
            " 16  Sour cream    421 non-null    int64  \n",
            " 17  Pork          421 non-null    int64  \n",
            " 18  Chicken       421 non-null    int64  \n",
            " 19  Shrimp        421 non-null    int64  \n",
            " 20  Fish          421 non-null    int64  \n",
            " 21  Rice          421 non-null    int64  \n",
            " 22  Beans         421 non-null    int64  \n",
            " 23  Lettuce       421 non-null    int64  \n",
            " 24  Tomato        421 non-null    int64  \n",
            " 25  Bell peper    421 non-null    int64  \n",
            " 26  Cabbage       421 non-null    int64  \n",
            " 27  Sauce         421 non-null    int64  \n",
            " 28  Salsa.1       421 non-null    int64  \n",
            " 29  Cilantro      421 non-null    int64  \n",
            " 30  Onion         421 non-null    int64  \n",
            " 31  Pineapple     421 non-null    int64  \n",
            " 32  Avocado       421 non-null    int64  \n",
            " 33  Great         421 non-null    int64  \n",
            "dtypes: float64(12), int64(22)\n",
            "memory usage: 115.1 KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.567696\n",
              "1    0.432304\n",
              "Name: Great, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m3MS-e6ZiJB"
      },
      "source": [
        "# Work for the stretch Goal\n",
        "df['Burrito'].value_counts()\n",
        "df['Burrito'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ylEsECY6Rt"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfh_ElDeY6Rt"
      },
      "source": [
        "target= 'Great'\n",
        "\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KHDLAbVY6Ru"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm0Kw7MrZrGc",
        "outputId": "b3d51afb-83ad-4824-95d8-5cb7dd3f1e1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Do some eda to determine the year ranges we are working with \n",
        "print(df.index.year.min())\n",
        "print(df.index.year.max())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2011\n",
            "2026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM_3JYhPY6Ru"
      },
      "source": [
        "# Create and apply a mask to get the date ranges you want.\n",
        "range_train = 2016\n",
        "range_test = 2018\n",
        "mask_train = (X.index.year >= range_train) & (X.index.year < range_test)\n",
        "mask_test = (X.index.year >= range_test)\n",
        "\n",
        "X_train, y_train = X.loc[mask_train], y.loc[mask_train]\n",
        "X_test, y_test = X.loc[mask_test], y.loc[mask_test]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne4Mpp84Z3bC",
        "outputId": "37370e8b-31f0-490d-8dc8-512ab440b4f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the shape of each set\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(381, 33)\n",
            "(38, 33)\n",
            "            Cost  Hunger  Volume  Tortilla  ...  Cilantro  Onion  Pineapple  Avocado\n",
            "Date                                        ...                                     \n",
            "2016-01-18  6.49     3.0     0.0       3.0  ...         0      0          0        0\n",
            "2016-01-24  5.45     3.5     0.0       2.0  ...         0      0          0        0\n",
            "2016-01-24  4.85     1.5     0.0       3.0  ...         0      0          0        0\n",
            "2016-01-24  5.25     2.0     0.0       3.0  ...         0      0          0        0\n",
            "2016-01-27  6.59     4.0     0.0       4.0  ...         0      0          0        0\n",
            "\n",
            "[5 rows x 33 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClZYD2CbY6Ru"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpRm6wCfY6Ru",
        "outputId": "9e023012-63cc-49f8-fbe1-f806e9d40cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Accuracy Score: 0.5826771653543307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb15AE_UY6Ru"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjSH1AYY6Ru",
        "outputId": "acb83ab4-e640-41e2-907d-a03a5b22ddd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_logr = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
        "                           SimpleImputer(strategy='mean'),\n",
        "                           StandardScaler(),\n",
        "                           LogisticRegression())\n",
        "model_logr.fit(X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
            "  elif pd.api.types.is_categorical(cols):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=[], drop_invariant=False,\n",
              "                               handle_missing='value', handle_unknown='value',\n",
              "                               return_df=True, use_cat_names=True, verbose=0)),\n",
              "                ('simpleimputer',\n",
              "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
              "                               missing_values=nan, strategy='mean',\n",
              "                               verbose=0)),\n",
              "                ('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMdoKR5jY6Ru"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipk_cmw_Y6Rv",
        "outputId": "dfdbd38c-fed3-41f8-938a-974598c17991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_acc = model_logr.score(X_train, y_train)\n",
        "test_acc = model_logr.score(X_test, y_test)\n",
        "\n",
        "print('Training MAE:', training_acc)\n",
        "print('Test MAE:', test_acc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MAE: 0.8713910761154856\n",
            "Test MAE: 0.7894736842105263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3prRCTOY6Rv"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwyukltZY6Rv",
        "outputId": "a574ced1-12bd-4be3-a3e1-e0ef36618d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Create your horizontal barchart here.\n",
        "\n",
        "coefficients = model_logr.named_steps['logisticregression'].coef_[0]\n",
        "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
        "feature_importance = pd.Series(coefficients, index= features).sort_values(key=abs)\n",
        "feature_importance\n",
        "\n",
        "feature_importance.tail(10).plot(kind= 'barh');"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dah5TYbAY6Rv"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GPOrtb6Y6Rv",
        "outputId": "cf35c477-c2db-4590-d528-460a8b7ec345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "y_pred = model_logr.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDsC6W5Gr3Q8",
        "outputId": "97899aa5-6db8-4081-e855-1a9b91dce3c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "y_pred_prob = model_logr.predict_proba(X_test)\n",
        "y_pred_prob"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00040953, 0.99959047],\n",
              "       [0.00849687, 0.99150313],\n",
              "       [0.9961651 , 0.0038349 ],\n",
              "       [0.00006308, 0.99993692],\n",
              "       [0.97096308, 0.02903692],\n",
              "       [0.95286934, 0.04713066],\n",
              "       [0.20546101, 0.79453899],\n",
              "       [0.00598474, 0.99401526],\n",
              "       [0.62537325, 0.37462675],\n",
              "       [0.24595556, 0.75404444],\n",
              "       [0.92487509, 0.07512491],\n",
              "       [0.99358502, 0.00641498],\n",
              "       [0.47926479, 0.52073521],\n",
              "       [0.49569008, 0.50430992],\n",
              "       [0.08611696, 0.91388304],\n",
              "       [0.14801261, 0.85198739],\n",
              "       [0.25393341, 0.74606659],\n",
              "       [0.97201564, 0.02798436],\n",
              "       [0.96832892, 0.03167108],\n",
              "       [0.97648888, 0.02351112],\n",
              "       [0.9503563 , 0.0496437 ],\n",
              "       [0.0473564 , 0.9526436 ],\n",
              "       [0.88487138, 0.11512862],\n",
              "       [0.25571647, 0.74428353],\n",
              "       [0.09487798, 0.90512202],\n",
              "       [0.76100441, 0.23899559],\n",
              "       [0.99935952, 0.00064048],\n",
              "       [0.00052693, 0.99947307],\n",
              "       [0.02702081, 0.97297919],\n",
              "       [0.00639541, 0.99360459],\n",
              "       [0.90928033, 0.09071967],\n",
              "       [0.10390676, 0.89609324],\n",
              "       [0.02006051, 0.97993949],\n",
              "       [0.99851877, 0.00148123],\n",
              "       [0.87190001, 0.12809999],\n",
              "       [0.33168931, 0.66831069],\n",
              "       [0.00316873, 0.99683127],\n",
              "       [0.02821195, 0.97178805]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19B8T_SkY6Rv"
      },
      "source": [
        "**Give your written answer here:**\n",
        "The .predict() method will return a binary prediction of 1 or 0, based on your model. \n",
        "```\n",
        "The .predict() method will return a binary prediction of 1 or 0, based on your model. \n",
        "\n",
        "The .predict_proba() method will return the probability of your output being 1 or 0, which can be read as a percentage.\n",
        "\n",
        "\n",
        "```"
      ]
    }
  ]
}